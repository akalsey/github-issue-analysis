#!/usr/bin/env python3
"""
GitHub Issues Cycle Time Analyzer

Analyzes GitHub issues data from JSON files generated by sync_issues.py,
calculates cycle times, and generates comprehensive reports.
"""
# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "pandas",
#     "matplotlib",
#     "seaborn",
#     "python-dotenv",
#     "openai",
#     "rich",
# ]
# ///

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
import re
from dataclasses import dataclass
from pathlib import Path
from dotenv import load_dotenv
import argparse
import sys
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from rich.console import Console
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


@dataclass
class StageSegment:
    """Container for stage-based workflow tracking"""
    stage_name: str
    start_time: datetime
    end_time: Optional[datetime]
    duration_days: Optional[float]
    stage_type: str  # 'created', 'planning', 'development', 'review', 'testing', 'deployment', 'closed'
    is_work_time: bool  # True for active work stages, False for waiting stages
    
    # Common stage types:
    # - 'created': Issue created, waiting for planning
    # - 'planning': Requirements review, design, estimation
    # - 'development': Active coding/implementation
    # - 'review': Code review, feedback cycles
    # - 'testing': QA, validation, bug fixes
    # - 'deployment': Release preparation, deployment
    # - 'closed': Issue completed

def is_strategic_work(issue: Dict) -> bool:
    """
    Filter for strategic business value work vs operational maintenance.
    
    INCLUDE: product work, features, customer issues, epics
    EXCLUDE: chores, deployments, infrastructure, compliance tasks
    """
    labels_str = str(issue.get('labels', [])).lower()
    
    # INCLUDE: Strategic business value work
    include_patterns = [
        'product/',      # All product work (voice, messaging, ai, video, etc.)
        'epic',          # Major strategic initiatives
        'area/customer', # Customer-impacting issues
        'type/feature',  # New functionality/capabilities  
        'type/bug',      # Customer-affecting defects
    ]
    
    # EXCLUDE: Operational/maintenance work
    exclude_patterns = [
        'type/chore',     # Maintenance, deployments, cleanup
        'dev/iac',        # Infrastructure as code
        'deploy/',        # Deployment tasks
        'compliance',     # Regulatory/security tasks
        'tech-backlog',   # Technical debt
        'status/',        # Workflow states, not deliverables
        'area/internal',  # Internal tooling
    ]
    
    # Check for exclusion patterns first (higher priority)
    for pattern in exclude_patterns:
        if pattern in labels_str:
            return False
    
    # Check for inclusion patterns
    for pattern in include_patterns:
        if pattern in labels_str:
            return True
    
    # Default: exclude unlabeled or unclear work
    return False

@dataclass
class CycleTimeMetrics:
    """Container for cycle time calculations"""
    issue_number: int
    title: str
    created_at: datetime
    closed_at: Optional[datetime]
    work_started_at: Optional[datetime]
    lead_time_days: Optional[float]
    cycle_time_days: Optional[float]
    labels: List[str]
    assignee: Optional[str]
    milestone: Optional[str]
    state: str
    # Enhanced stage-based workflow analysis
    stage_segments: Optional[List[StageSegment]] = None
    # GitHub Projects data
    project_title: Optional[str] = None
    project_status: Optional[str] = None
    project_iteration: Optional[str] = None
    project_assignees: Optional[List[str]] = None
    total_work_time_days: Optional[float] = None
    total_wait_time_days: Optional[float] = None
    work_efficiency_ratio: Optional[float] = None

class GitHubCycleTimeAnalyzer:
    """Analyze cycle times for GitHub repository issues from JSON data"""
    
    def __init__(self, owner: str = None, repo: str = None):
        self.owner = owner
        self.repo = repo
        self.last_analyzed_metrics = []  # Store for visualization access
    
    def load_cycle_data_from_json(self, json_file_path: str) -> Dict:
        """Load cycle time data from JSON file"""
        with open(json_file_path, 'r') as f:
            data = json.load(f)
        
        # Extract repository metadata if available
        if 'repository' in data:
            repo_info = data['repository']
            self.owner = repo_info.get('github_owner')
            self.repo = repo_info.get('github_repo')
            print(f"📂 Loaded data for {self.owner}/{self.repo}")
            print(f"📅 Data synced: {repo_info.get('sync_date', 'Unknown')}")
            print(f"📊 Issues in dataset: {repo_info.get('total_issues_synced', len(data.get('issues', [])))}")
        
        return data
    
    
    def extract_work_start_date(self, issue: Dict) -> Optional[datetime]:
        """Determine when work actually started on an issue using data from JSON"""
        issue_number = issue['number']
        created_at = datetime.fromisoformat(issue['created_at'].replace('Z', '+00:00'))
        
        work_start_candidates = []
        
        # Get events from JSON data (if available)
        events = issue.get('timeline_events', [])
        
        # Check assignment date from events
        if issue.get('assignee') and events:
            try:
                for event in events:
                    if event.get('event') == 'assigned':
                        assigned_at = datetime.fromisoformat(event['created_at'].replace('Z', '+00:00'))
                        work_start_candidates.append(assigned_at)
                        break
            except Exception:
                pass
        
        # Check first commit date from JSON data (if available)
        commits = issue.get('commits', [])
        if commits:
            try:
                first_commit_date = datetime.fromisoformat(
                    commits[0]['commit']['committer']['date'].replace('Z', '+00:00')
                )
                work_start_candidates.append(first_commit_date)
            except Exception:
                pass
        
        # Check for labeled as "in progress" or similar from events
        if events:
            try:
                for event in events:
                    if (event.get('event') == 'labeled' and 
                        event.get('label', {}).get('name', '').lower() in 
                        ['in progress', 'in-progress', 'started', 'working']):
                        labeled_at = datetime.fromisoformat(event['created_at'].replace('Z', '+00:00'))
                        work_start_candidates.append(labeled_at)
                        break
            except Exception:
                pass
        
        # Return the earliest valid work start date
        # Work start must be after creation and before closure (if closed)
        closed_at = None
        if issue.get('closed_at'):
            closed_at = datetime.fromisoformat(issue['closed_at'].replace('Z', '+00:00'))
        
        valid_dates = []
        for date in work_start_candidates:
            if date >= created_at:  # Must be after creation
                if closed_at is None or date <= closed_at:  # Must be before closure (if closed)
                    valid_dates.append(date)
        
        return min(valid_dates) if valid_dates else None
    
    def _extract_assignment_date(self, events: List[Dict]) -> Optional[datetime]:
        """Extract the first assignment date from events"""
        for event in events:
            if event.get('event') == 'assigned':
                return datetime.fromisoformat(event['created_at'].replace('Z', '+00:00'))
        return None
    
    def _extract_first_commit_date(self, commits: List[Dict]) -> Optional[datetime]:
        """Extract the first commit date"""
        if commits:
            commit_dates = []
            for commit in commits:
                if commit.get('commit', {}).get('author', {}).get('date'):
                    date_str = commit['commit']['author']['date']
                    commit_dates.append(datetime.fromisoformat(date_str.replace('Z', '+00:00')))
            return min(commit_dates) if commit_dates else None
        return None
    
    def _extract_first_pr_date(self, prs: List[Dict]) -> Optional[datetime]:
        """Extract the first PR creation date"""
        if prs:
            pr_dates = []
            for pr in prs:
                if pr.get('created_at'):
                    pr_dates.append(datetime.fromisoformat(pr['created_at'].replace('Z', '+00:00')))
            return min(pr_dates) if pr_dates else None
        return None
    
    def _determine_stage_type(self, current_milestone: str, next_milestone: str) -> Tuple[str, str]:
        """Determine stage type (wait/work) and name based on milestone transitions"""
        # Define stage progressions
        stage_mapping = {
            ('created', 'assigned'): ('wait', 'Requirement Review'),
            ('created', 'development_started'): ('wait', 'Planning & Assignment'),
            ('created', 'review_started'): ('wait', 'Planning & Development'),
            ('created', 'closed'): ('wait', 'Complete Lifecycle'),
            ('assigned', 'development_started'): ('work', 'Development Planning'),
            ('assigned', 'review_started'): ('work', 'Development'),
            ('assigned', 'closed'): ('work', 'Development & Deployment'),
            ('development_started', 'review_started'): ('work', 'Active Development'),
            ('development_started', 'closed'): ('work', 'Development & Integration'),
            ('review_started', 'closed'): ('wait', 'Code Review & Deployment'),
        }
        
        key = (current_milestone, next_milestone)
        return stage_mapping.get(key, ('wait', f'{current_milestone.title()} to {next_milestone.title()}'))
    
    def analyze_stage_segments(self, issue: Dict) -> List[StageSegment]:
        """Analyze stage progression for an issue: create -> wait -> stage -> wait -> another stage -> wait..."""
        segments = []
        issue_number = issue['number']
        
        try:
            # Get basic timeline data
            created_at = datetime.fromisoformat(issue['created_at'].replace('Z', '+00:00'))
            closed_at = None
            if issue.get('closed_at'):
                closed_at = datetime.fromisoformat(issue['closed_at'].replace('Z', '+00:00'))
            
            # Get events, commits, and PRs for milestone detection
            events = self.fetch_issue_events(issue_number)
            commits = self.fetch_commits_for_issue(issue_number)
            prs = self.fetch_pull_requests_for_issue(issue_number)
            
            # Extract key milestone dates
            assignment_date = self._extract_assignment_date(events)
            first_commit_date = self._extract_first_commit_date(commits)
            first_pr_date = self._extract_first_pr_date(prs)
            
            # Build stage progression timeline
            milestones = []
            
            # Always start with creation
            milestones.append(('created', created_at))
            
            # Add assignment if it exists
            if assignment_date:
                milestones.append(('assigned', assignment_date))
            
            # Add first commit if it exists
            if first_commit_date:
                milestones.append(('development_started', first_commit_date))
            
            # Add first PR if it exists
            if first_pr_date:
                milestones.append(('review_started', first_pr_date))
            
            # Add closure if closed
            if closed_at:
                milestones.append(('closed', closed_at))
            
            # Sort milestones by time
            milestones.sort(key=lambda x: x[1])
            
            # Create stage segments from milestone pairs
            for i in range(len(milestones) - 1):
                current_milestone, current_time = milestones[i]
                next_milestone, next_time = milestones[i + 1]
                
                # Determine stage type and name
                stage_type, stage_name = self._determine_stage_type(current_milestone, next_milestone)
                
                duration = (next_time - current_time).total_seconds() / (24 * 3600)
                
                segments.append(StageSegment(
                    stage_name=stage_name,
                    stage_type=stage_type,
                    start_time=current_time,
                    end_time=next_time,
                    duration_days=duration
                ))
            
            return segments
            
        except Exception as e:
            # Return empty list if analysis fails
            return []
    
    def calculate_cycle_times(self, issues: List[Dict], fast_mode: bool = False) -> List[CycleTimeMetrics]:
        """Calculate cycle time metrics for all issues"""
        metrics = []
        
        mode_text = " (fast mode - skipping work start detection)" if fast_mode else ""
        print(f"🔄 Calculating cycle times for {len(issues)} issues{mode_text}...")
        
        # Check what data is available and inform user
        if issues:
            sample_issue = issues[0]
            has_timeline = 'timeline_events' in sample_issue and sample_issue['timeline_events']
            has_commits = 'commits' in sample_issue and sample_issue['commits']  
            has_projects = 'project_data' in sample_issue and sample_issue['project_data']
            
            print(f"📊 Data availability:")
            print(f"   {'✅' if has_timeline else '❌'} Timeline events: {'Available' if has_timeline else 'Not available (limited work start detection)'}")
            print(f"   {'✅' if has_commits else '❌'} Commit data: {'Available' if has_commits else 'Not available (limited work start detection)'}")
            print(f"   {'✅' if has_projects else '❌'} Project data: {'Available' if has_projects else 'Not available (basic analysis only)'}")
        
        for i, issue in enumerate(issues):
            if i % 50 == 0:  # Show progress every 50 issues
                progress_percent = (i / len(issues)) * 100
                print(f"⚙️  Processing issue {i+1}/{len(issues)} ({progress_percent:.1f}%) - #{issue['number']}")
                
                created_at = datetime.fromisoformat(issue['created_at'].replace('Z', '+00:00'))
                closed_at = None
                if issue['closed_at']:
                    closed_at = datetime.fromisoformat(issue['closed_at'].replace('Z', '+00:00'))
                
                work_started_at = None if fast_mode else self.extract_work_start_date(issue)
                
                # Calculate lead time (creation to closure)
                lead_time_days = None
                if closed_at:
                    lead_time_days = (closed_at - created_at).total_seconds() / (24 * 3600)
                
                # Calculate cycle time (work start to closure)
                cycle_time_days = None
                if closed_at and work_started_at:
                    cycle_time_days = (closed_at - work_started_at).total_seconds() / (24 * 3600)
                    # Safety check: if cycle time is negative, something went wrong - set to None
                    if cycle_time_days < 0:
                        cycle_time_days = None
                
                labels = [label['name'] for label in issue.get('labels', [])]
                assignee = issue.get('assignee', {}).get('login') if issue.get('assignee') else None
                milestone = issue.get('milestone', {}).get('title') if issue.get('milestone') else None
                
                # Analyze stage segments for closed issues
                stage_segments = None
                total_work_time = None
                total_wait_time = None
                work_efficiency_ratio = None
                
                if closed_at:  # Only analyze completed issues
                    stage_segments = self.analyze_stage_segments(issue)
                    if stage_segments:
                        work_times = [seg.duration_days for seg in stage_segments if seg.stage_type == 'work' and seg.duration_days]
                        wait_times = [seg.duration_days for seg in stage_segments if seg.stage_type == 'wait' and seg.duration_days]
                        
                        total_work_time = sum(work_times) if work_times else 0
                        total_wait_time = sum(wait_times) if wait_times else 0
                        
                        if total_work_time + total_wait_time > 0:
                            work_efficiency_ratio = total_work_time / (total_work_time + total_wait_time)
                
                # Extract project data if available
                project_title = None
                project_status = None
                project_iteration = None
                project_assignees = None
                
                project_data = issue.get('project_data', [])
                if project_data:
                    # Use the first project if multiple projects exist
                    first_project = project_data[0]
                    project_title = first_project.get('project_title')
                    
                    # Extract common field values
                    field_values = first_project.get('field_values', {})
                    project_status = field_values.get('Status')
                    project_iteration = field_values.get('Iteration') or field_values.get('Sprint')
                    
                    # Try to get assignees from project data, fall back to issue assignees
                    if 'assignees' in first_project:
                        project_assignees = first_project['assignees']
                
                metrics.append(CycleTimeMetrics(
                    issue_number=issue['number'],
                    title=issue['title'],
                    created_at=created_at,
                    closed_at=closed_at,
                    work_started_at=work_started_at,
                    lead_time_days=lead_time_days,
                    cycle_time_days=cycle_time_days,
                    labels=labels,
                    assignee=assignee,
                    milestone=milestone,
                    state=issue['state'],
                    stage_segments=stage_segments,
                    total_work_time_days=total_work_time,
                    total_wait_time_days=total_wait_time,
                    work_efficiency_ratio=work_efficiency_ratio,
                    project_title=project_title,
                    project_status=project_status,
                    project_iteration=project_iteration,
                    project_assignees=project_assignees
                ))
        
        print(f"✅ Cycle time calculation complete: {len(metrics)} issues processed")
        return metrics
    
    def _calculate_monthly_cycle_trends(self, closed_issues: pd.DataFrame) -> pd.DataFrame:
        """Calculate monthly cycle time averages with rolling 6-month trends"""
        # Filter issues with cycle time data
        cycle_data = closed_issues[closed_issues['cycle_time_days'].notna()].copy()
        
        if cycle_data.empty:
            return pd.DataFrame()
        
        # Group by month based on closure date
        cycle_data['closed_month'] = pd.to_datetime(cycle_data['closed_at']).dt.to_period('M')
        monthly_avg = cycle_data.groupby('closed_month')['cycle_time_days'].agg(['mean', 'count']).reset_index()
        monthly_avg.columns = ['month', 'monthly_avg', 'issue_count']
        
        # Filter out months with very few issues (less than 3) for more reliable averages
        monthly_avg = monthly_avg[monthly_avg['issue_count'] >= 3]
        
        if len(monthly_avg) < 6:
            return pd.DataFrame()
        
        # Set month as index and calculate 6-month rolling average
        monthly_avg = monthly_avg.set_index('month')
        monthly_avg['rolling_6m'] = monthly_avg['monthly_avg'].rolling(window=6, min_periods=3).mean()
        
        # Convert period index to timestamp for plotting
        monthly_avg.index = monthly_avg.index.to_timestamp()
        
        return monthly_avg
    
    def _extract_issue_type(self, labels: List[str]) -> str:
        """Extract issue type from labels"""
        type_labels = [label for label in labels if label.startswith('type/')]
        if type_labels:
            return type_labels[0].replace('type/', '')
        return 'untyped'
    
    def _extract_team(self, labels: List[str]) -> str:
        """Extract team from labels"""
        team_labels = [label for label in labels if label.startswith('team/')]
        if team_labels:
            return team_labels[0].replace('team/', '')
        return 'unassigned'
    
    def _extract_product_area(self, labels: List[str]) -> str:
        """Extract product area from labels"""
        product_labels = [label for label in labels if label.startswith('product/')]
        if product_labels:
            return product_labels[0].replace('product/', '')
        return 'unspecified'
    
    def _extract_priority(self, labels: List[str]) -> str:
        """Extract priority from labels"""
        priority_labels = [label for label in labels if label.upper() in ['P0', 'P1', 'P2', 'P3', 'P4']]
        if priority_labels:
            return priority_labels[0].upper()
        security_labels = [label for label in labels if 'security' in label.lower()]
        if security_labels:
            return 'SECURITY'
        return 'normal'
    
    def _analyze_cycle_time_segments(self, df: pd.DataFrame) -> Dict:
        """Analyze cycle times by different segments"""
        closed_issues = df[df['state'] == 'closed'].copy()
        
        if len(closed_issues) == 0:
            return {}
        
        # Extract categorization data
        closed_issues['issue_type'] = closed_issues['labels'].apply(
            lambda x: self._extract_issue_type(x.split(', ') if x else [])
        )
        closed_issues['team'] = closed_issues['labels'].apply(
            lambda x: self._extract_team(x.split(', ') if x else [])
        )
        closed_issues['product_area'] = closed_issues['labels'].apply(
            lambda x: self._extract_product_area(x.split(', ') if x else [])
        )
        closed_issues['priority'] = closed_issues['labels'].apply(
            lambda x: self._extract_priority(x.split(', ') if x else [])
        )
        
        analysis = {}
        
        # Cycle time by issue type
        cycle_time_data = closed_issues['cycle_time_days'].dropna()
        if not cycle_time_data.empty:
            type_analysis = closed_issues[closed_issues['cycle_time_days'].notna()].groupby('issue_type')['cycle_time_days'].agg(['count', 'mean', 'median']).round(1)
            type_analysis = type_analysis[type_analysis['count'] >= 2]  # Filter out single-issue types
            analysis['by_issue_type'] = type_analysis.to_dict('index')
        else:
            analysis['by_issue_type'] = {}
        
        # Cycle time by team
        if not cycle_time_data.empty:
            team_analysis = closed_issues[closed_issues['cycle_time_days'].notna()].groupby('team')['cycle_time_days'].agg(['count', 'mean', 'median']).round(1)
            team_analysis = team_analysis[team_analysis['count'] >= 2]
            analysis['by_team'] = team_analysis.to_dict('index')
        else:
            analysis['by_team'] = {}
        
        # Cycle time by product area
        if not cycle_time_data.empty:
            product_analysis = closed_issues[closed_issues['cycle_time_days'].notna()].groupby('product_area')['cycle_time_days'].agg(['count', 'mean', 'median']).round(1)
            product_analysis = product_analysis[product_analysis['count'] >= 2]
            analysis['by_product_area'] = product_analysis.to_dict('index')
        else:
            analysis['by_product_area'] = {}
        
        # Cycle time by priority
        if not cycle_time_data.empty:
            priority_analysis = closed_issues[closed_issues['cycle_time_days'].notna()].groupby('priority')['cycle_time_days'].agg(['count', 'mean', 'median']).round(1)
            priority_analysis = priority_analysis[priority_analysis['count'] >= 1]  # Keep single P1/security issues
            analysis['by_priority'] = priority_analysis.to_dict('index')
        else:
            analysis['by_priority'] = {}
        
        return analysis
    
    def _analyze_assignment_patterns(self, df: pd.DataFrame) -> Dict:
        """Analyze assignment patterns for workflow insights"""
        analysis = {}
        
        # Simplified analysis without API calls
        analysis['time_to_assignment'] = {'mean_days': 'N/A', 'median_days': 'N/A', 'max_days': 'N/A', 'count': 0}
        analysis['assignment_stability'] = {'mean_reassignments': 'N/A', 'issues_with_reassignments': 0, 'total_issues_analyzed': 0, 'stability_rate': 'N/A'}
        
        # Team collaboration analysis (issues with multiple assignees)
        multi_assignee_issues = len(df[df['assignee'].str.contains(',', na=False)]) if 'assignee' in df.columns else 0
        total_assigned_issues = len(df[df['assignee'].notna()]) if 'assignee' in df.columns else 0
        
        analysis['team_collaboration'] = {
            'multi_assignee_issues': multi_assignee_issues,
            'total_assigned_issues': total_assigned_issues,
            'collaboration_rate': round(multi_assignee_issues / total_assigned_issues * 100, 1) if total_assigned_issues > 0 else 0
        }
        
        return analysis
    
    def _analyze_status_progression(self, df: pd.DataFrame) -> Dict:
        """Analyze time spent in different status states"""
        analysis = {}
        
        queue_times = []  # Time from creation to work start
        
        for _, issue in df.iterrows():
            try:
                # Analyze queue time (creation to work start) using available data
                if issue['work_started_at'] and issue['created_at']:
                    created_at = datetime.fromisoformat(issue['created_at'].isoformat())
                    work_started_at = datetime.fromisoformat(issue['work_started_at'].isoformat())
                    queue_time = (work_started_at - created_at).total_seconds() / (24 * 3600)
                    queue_times.append(queue_time)
                    
            except Exception:
                # Skip if we can't analyze this issue
                continue
        
        # Status progression analysis (simplified without API calls)
        analysis['needs_review_time'] = {'mean_days': 'N/A', 'median_days': 'N/A', 'max_days': 'N/A', 'count': 0}
        
        if queue_times:
            analysis['queue_time'] = {
                'mean_days': round(sum(queue_times) / len(queue_times), 1),
                'median_days': round(sorted(queue_times)[len(queue_times)//2], 1),
                'max_days': round(max(queue_times), 1),
                'count': len(queue_times)
            }
        else:
            analysis['queue_time'] = {'mean_days': 'N/A', 'median_days': 'N/A', 'max_days': 'N/A', 'count': 0}
        
        return analysis
    
    def _create_timeline_visualization(self, df: pd.DataFrame, output_dir: str):
        """Create timeline visualization showing stage progression trends over time"""
        closed_issues = df[df['state'] == 'closed'].copy()
        
        if len(closed_issues) == 0:
            return
        
        # Extract issue types and add closure month for trend analysis
        closed_issues['issue_type'] = closed_issues['labels'].apply(
            lambda x: self._extract_issue_type(x.split(', ') if x else [])
        )
        
        # Add closure month for trend analysis
        closed_issues['closed_at'] = pd.to_datetime(closed_issues['closed_at'])
        closed_issues['closure_month'] = closed_issues['closed_at'].dt.to_period('M')
        
        # Filter to issue types with at least 5 issues for meaningful trends
        type_counts = closed_issues['issue_type'].value_counts()
        valid_types = type_counts[type_counts >= 5].index.tolist()
        closed_issues = closed_issues[closed_issues['issue_type'].isin(valid_types)]
        
        if len(closed_issues) == 0:
            return
        
        # Create timeline data using stage segments
        timeline_data = []
        metrics_by_issue = {metric.issue_number: metric for metric in self.last_analyzed_metrics 
                           if metric.stage_segments and metric.state == 'closed'}
        
        for _, issue in closed_issues.iterrows():
            issue_number = issue['issue_number']
            if issue_number in metrics_by_issue:
                metric = metrics_by_issue[issue_number]
                
                # Aggregate stage data by type (work vs wait)
                total_work_time = 0
                total_wait_time = 0
                stage_breakdown = {}
                
                for segment in metric.stage_segments:
                    duration = segment.duration_days or 0
                    stage_name = segment.stage_name
                    
                    # Track individual stages
                    if stage_name not in stage_breakdown:
                        stage_breakdown[stage_name] = 0
                    stage_breakdown[stage_name] += duration
                    
                    # Aggregate by work/wait type
                    if segment.stage_type == 'work':
                        total_work_time += duration
                    else:
                        total_wait_time += duration
                
                total_time = total_work_time + total_wait_time
                efficiency_ratio = total_work_time / total_time if total_time > 0 else 0
                
                timeline_entry = {
                    'issue_type': issue['issue_type'],
                    'queue_time': total_wait_time,  # Renamed for compatibility
                    'work_time': total_work_time,   # Renamed for compatibility
                    'total_time': total_time,
                    'closure_month': issue['closure_month'],
                    'efficiency_ratio': efficiency_ratio
                }
                
                # Add individual stage data
                timeline_entry.update(stage_breakdown)
                timeline_data.append(timeline_entry)
        
        if not timeline_data:
            return
        
        timeline_df = pd.DataFrame(timeline_data)
        
        # Create temporal trend visualization (3 panels)
        fig, axes = plt.subplots(3, 1, figsize=(15, 16))
        fig.suptitle(f'Stage Progression Trends - {self.owner}/{self.repo}', fontsize=16, fontweight='bold')
        
        # 1. Monthly wait time trends by issue type (last 12 months)
        print("\n📈 Analyzing temporal trends in stage progression times...")
        
        # Get last 12 months of data
        latest_month = timeline_df['closure_month'].max()
        twelve_months_ago = latest_month - 11  # 12 months including current
        recent_data = timeline_df[timeline_df['closure_month'] >= twelve_months_ago]
        
        # 1. Monthly stage progression stacked bar chart
        self._create_monthly_stage_progression_chart(timeline_df, recent_data, axes[0])
        
        # 2. Monthly stage progression by issue type stacked bar chart  
        self._create_monthly_stage_by_type_chart(timeline_df, recent_data, axes[1])
        
        # 3. Detailed stage breakdown by issue type
        self._create_detailed_stage_breakdown_chart(timeline_df, axes[2])
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/timeline_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate trend insights for console output
        self._analyze_wait_time_trends(timeline_df)
        
        return timeline_df
    
    def _analyze_wait_time_trends(self, timeline_df: pd.DataFrame):
        """Analyze and report wait time trends"""
        if timeline_df.empty:
            return
        
        print("\n📊 Wait Time Trend Insights:")
        print("=" * 40)
        
        # Overall trend analysis
        latest_month = timeline_df['closure_month'].max()
        six_months_ago = latest_month - 5
        twelve_months_ago = latest_month - 11
        
        # Compare recent 6 months vs previous 6 months
        recent_6m = timeline_df[timeline_df['closure_month'] >= six_months_ago]
        previous_6m = timeline_df[
            (timeline_df['closure_month'] >= twelve_months_ago) & 
            (timeline_df['closure_month'] < six_months_ago)
        ]
        
        if not recent_6m.empty and not previous_6m.empty:
            recent_wait = recent_6m['queue_time'].mean()
            previous_wait = previous_6m['queue_time'].mean()
            recent_efficiency = recent_6m['efficiency_ratio'].mean()
            previous_efficiency = previous_6m['efficiency_ratio'].mean()
            
            wait_change = recent_wait - previous_wait
            efficiency_change = recent_efficiency - previous_efficiency
            
            print(f"📈 Overall Trends (6-month comparison):")
            print(f"  Wait Time: {previous_wait:.1f}d → {recent_wait:.1f}d ({wait_change:+.1f}d)")
            print(f"  Efficiency: {previous_efficiency:.2%} → {recent_efficiency:.2%} ({efficiency_change:+.2%})")
            
            # Determine overall trend
            if wait_change > 2:
                print(f"  🚨 ALERT: Wait times are increasing significantly")
            elif wait_change < -2:
                print(f"  ✅ GOOD: Wait times are decreasing")
            else:
                print(f"  ➡️  STABLE: Wait times are relatively stable")
            
            # Efficiency trend
            if efficiency_change < -0.05:
                print(f"  ⚠️  WARNING: Work efficiency is declining")
            elif efficiency_change > 0.05:
                print(f"  📈 IMPROVING: Work efficiency is increasing")
            
        # Identify problematic issue types
        print(f"\n🎯 Issue Type Performance:")
        type_analysis = timeline_df.groupby('issue_type').agg({
            'queue_time': ['mean', 'count'],
            'efficiency_ratio': 'mean'
        }).round(2)
        
        # Flatten column names
        type_analysis.columns = ['avg_wait', 'count', 'avg_efficiency']
        type_analysis = type_analysis[type_analysis['count'] >= 5].sort_values('avg_wait', ascending=False)
        
        print(f"  Highest Wait Times:")
        for issue_type, data in type_analysis.head(3).iterrows():
            print(f"    • {issue_type}: {data['avg_wait']:.1f}d wait, {data['avg_efficiency']:.1%} efficiency (n={int(data['count'])})")
        
        print(f"  Best Efficiency:")
        best_efficiency = type_analysis.sort_values('avg_efficiency', ascending=False)
        for issue_type, data in best_efficiency.head(3).iterrows():
            print(f"    • {issue_type}: {data['avg_efficiency']:.1%} efficiency, {data['avg_wait']:.1f}d wait (n={int(data['count'])})")
    
    def _generate_ai_recommendations(self, df: pd.DataFrame, lead_time_stats: pd.Series, 
                                   cycle_time_stats: pd.Series, monthly_cycle_data: pd.DataFrame) -> List[str]:
        """Generate AI-powered recommendations based on cycle time analysis"""
        openai_api_key = os.getenv('OPENAI_API_KEY')
        openai_model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
        
        if not OPENAI_AVAILABLE or not openai_api_key:
            return [
                "Focus on reducing queue time (creation to work start)",
                "Identify and address bottlenecks in high-cycle-time issues",
                "Consider breaking down large issues (>90th percentile cycle time)",
                "Implement clearer work-in-progress tracking"
            ]
        
        try:
            # Prepare analysis data
            closed_issues = df[df['state'] == 'closed']
            
            analysis_summary = {
                "repository": f"{self.owner}/{self.repo}",
                "total_issues": len(df),
                "closed_issues": len(closed_issues),
                "lead_time_avg": round(lead_time_stats['mean'], 1) if not lead_time_stats.empty else "N/A",
                "lead_time_median": round(lead_time_stats['50%'], 1) if not lead_time_stats.empty else "N/A",
                "cycle_time_avg": round(cycle_time_stats['mean'], 1) if not cycle_time_stats.empty else "N/A",
                "cycle_time_median": round(cycle_time_stats['50%'], 1) if not cycle_time_stats.empty else "N/A",
                "issues_without_assignee": len(df[df['assignee'].isna()]),
                "issues_with_cycle_time": len(df[df['cycle_time_days'].notna()]),
                "avg_comments": round(df['comments'].mean(), 1) if 'comments' in df.columns and not df.empty and df['comments'].notna().any() else "N/A",
                "monthly_trend": "improving" if not monthly_cycle_data.empty and len(monthly_cycle_data) >= 2 and monthly_cycle_data['rolling_6m'].iloc[-1] < monthly_cycle_data['rolling_6m'].iloc[-2] else "stable/worsening",
            }
            
            # Get top assignees by cycle time
            if not cycle_time_stats.empty and len(closed_issues[closed_issues['cycle_time_days'].notna()]) > 0:
                assignee_stats = closed_issues[closed_issues['cycle_time_days'].notna()].groupby('assignee')['cycle_time_days'].agg(['mean', 'count']).sort_values('mean')
                analysis_summary["top_performers"] = assignee_stats.head(3).to_dict() if not assignee_stats.empty else "N/A"
                analysis_summary["bottlenecks"] = assignee_stats.tail(3).to_dict() if not assignee_stats.empty else "N/A"
            else:
                analysis_summary["top_performers"] = "N/A"
                analysis_summary["bottlenecks"] = "N/A"
            
            # Create prompt for AI
            prompt = f"""
            Analyze this GitHub repository's issue cycle time data and provide 4-6 specific, actionable recommendations to improve development velocity and reduce cycle times.

            Data Summary:
            - Repository: {analysis_summary['repository']}
            - Total Issues: {analysis_summary['total_issues']}
            - Closed Issues: {analysis_summary['closed_issues']}
            - Average Lead Time: {analysis_summary['lead_time_avg']} days
            - Average Cycle Time: {analysis_summary['cycle_time_avg']} days
            - Issues without assignee: {analysis_summary['issues_without_assignee']}
            - Issues with detectable work start: {analysis_summary['issues_with_cycle_time']}
            - Monthly trend: {analysis_summary['monthly_trend']}

            Focus on:
            1. Process improvements based on the data patterns
            2. Workflow bottlenecks and how to address them
            3. Assignment and work-in-progress practices
            4. Specific metrics-driven suggestions

            Return only a bullet-point list of recommendations, no other text.
            """

            client = openai.OpenAI(api_key=openai_api_key)
            response = client.chat.completions.create(
                model=openai_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.7
            )
            
            recommendations_text = response.choices[0].message.content.strip()
            
            # Parse into list
            recommendations = []
            for line in recommendations_text.split('\n'):
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('•') or line.startswith('*')):
                    recommendations.append(line[1:].strip())
                elif line and not line.startswith('#'):
                    recommendations.append(line)
            
            return recommendations[:6]  # Limit to 6 recommendations
            
        except Exception as e:
            print(f"Failed to generate AI recommendations: {e}")
            return [
                "Focus on reducing queue time (creation to work start)",
                "Identify and address bottlenecks in high-cycle-time issues",
                "Consider breaking down large issues (>90th percentile cycle time)",
                "Implement clearer work-in-progress tracking"
            ]
    
    def analyze_project_workflow(self, metrics: List[CycleTimeMetrics]):
        """Analyze GitHub Projects workflow efficiency"""
        workflow_data = []
        
        for metric in metrics:
            if metric.project_title and metric.state == 'open':
                workflow_data.append({
                    'issue_number': metric.issue_number,
                    'title': metric.title,
                    'product_area': self._get_product_area_from_labels(metric.labels),
                    'project_status': metric.project_status or 'Unknown',
                    'created_at': metric.created_at,
                    'assignee': metric.assignee,
                    'age_days': (datetime.now(timezone.utc) - metric.created_at).days
                })
        
        if not workflow_data:
            return None
            
        workflow_stages = [
            'Dev Backlog', 'Dev In Progress', 'Code Review', 
            'To Deploy', 'Verify in Production', 'Done'
        ]
        
        # Analyze workflow distribution
        status_distribution = {}
        total_issues = len(workflow_data)
        
        for stage in workflow_stages:
            count = sum(1 for item in workflow_data if item['project_status'] == stage)
            status_distribution[stage] = {
                'count': count,
                'percentage': (count / total_issues * 100) if total_issues > 0 else 0
            }
        
        # Analyze bottlenecks (>15% of work)
        bottlenecks = []
        bottleneck_threshold = total_issues * 0.15
        
        for stage in workflow_stages[:-1]:  # Exclude 'Done'
            if status_distribution[stage]['count'] > bottleneck_threshold:
                bottlenecks.append({
                    'stage': stage,
                    'count': status_distribution[stage]['count'],
                    'percentage': status_distribution[stage]['percentage']
                })
        
        # Analyze work age by stage
        age_analysis = {}
        for stage in workflow_stages[:-1]:
            stage_items = [item for item in workflow_data if item['project_status'] == stage]
            if stage_items:
                ages = [item['age_days'] for item in stage_items]
                age_analysis[stage] = {
                    'avg_age': sum(ages) / len(ages),
                    'max_age': max(ages),
                    'stale_count': sum(1 for age in ages if age > 30)
                }
        
        # Assignment analysis
        assigned_count = sum(1 for item in workflow_data if item['assignee'])
        unassigned_count = total_issues - assigned_count
        
        return {
            'total_issues': total_issues,
            'status_distribution': status_distribution,
            'bottlenecks': bottlenecks,
            'age_analysis': age_analysis,
            'assignment': {
                'assigned': assigned_count,
                'unassigned': unassigned_count,
                'unassigned_percentage': (unassigned_count / total_issues * 100) if total_issues > 0 else 0
            },
            'workflow_stages': workflow_stages,
            'workflow_data': workflow_data  # Include raw data for detailed analysis
        }
    
    def analyze_project_workflow_detailed(self, metrics: List[CycleTimeMetrics]):
        """Enhanced workflow analysis with detailed console output"""
        workflow_data = []
        
        for metric in metrics:
            if metric.project_title and metric.state == 'open':
                workflow_data.append({
                    'issue_number': metric.issue_number,
                    'title': metric.title,
                    'product_area': self._get_product_area_from_labels(metric.labels),
                    'project_status': metric.project_status or 'Unknown',
                    'created_at': metric.created_at,
                    'labels': metric.labels,
                    'assignee': metric.assignee
                })
        
        if not workflow_data:
            print("No project workflow data found")
            return None
        
        import pandas as pd
        import numpy as np
        df = pd.DataFrame(workflow_data)
        
        # Define workflow stages in order
        workflow_stages = [
            'Dev Backlog',
            'Dev In Progress', 
            'Code Review',
            'To Deploy',
            'Verify in Production',
            'Done'
        ]
        
        print("🔄 GitHub Projects Workflow Analysis")
        print("=" * 50)
        
        # 1. Current Work Distribution
        print("\n📊 Current Work Distribution by Stage:")
        status_counts = df['project_status'].value_counts()
        for status in workflow_stages:
            count = status_counts.get(status, 0)
            percentage = (count / len(df) * 100) if len(df) > 0 else 0
            print(f"  {status:<20}: {count:>3} issues ({percentage:>5.1f}%)")
        
        # 2. Bottleneck Analysis
        print("\n🚨 Potential Bottlenecks:")
        bottleneck_threshold = len(df) * 0.15  # More than 15% of total work
        
        for status in workflow_stages[:-1]:  # Exclude 'Done'
            count = status_counts.get(status, 0)
            if count > bottleneck_threshold:
                print(f"  ⚠️  {status}: {count} issues ({count/len(df)*100:.1f}% of total work)")
        
        # 3. Work Distribution by Product Area
        print("\n🏗️  Work Distribution by Product Area:")
        product_status = df.groupby(['product_area', 'project_status']).size().unstack(fill_value=0)
        
        for area in product_status.index:
            print(f"\n  {area}:")
            for status in workflow_stages:
                if status in product_status.columns:
                    count = product_status.loc[area, status]
                    if count > 0:
                        print(f"    {status:<20}: {count:>2} issues")
        
        # 4. Age Analysis by Stage
        print("\n⏰ Work Age Analysis (days since created):")
        today = datetime.now(timezone.utc)
        
        for status in workflow_stages[:-1]:  # Exclude 'Done'
            stage_issues = df[df['project_status'] == status]
            if not stage_issues.empty:
                ages = [(today - created).days for created in stage_issues['created_at']]
                avg_age = np.mean(ages)
                max_age = max(ages)
                print(f"  {status:<20}: avg {avg_age:>5.1f} days, oldest {max_age:>3} days")
        
        # 5. Assignment Analysis
        print("\n👥 Assignment Status:")
        assigned_count = df[df['assignee'].notna()].shape[0]
        unassigned_count = df[df['assignee'].isna()].shape[0]
        
        print(f"  Assigned:   {assigned_count:>3} issues ({assigned_count/len(df)*100:>5.1f}%)")
        print(f"  Unassigned: {unassigned_count:>3} issues ({unassigned_count/len(df)*100:>5.1f}%)")
        
        # 6. Stale Work Analysis
        print("\n🕰️  Stale Work (>30 days old):")
        stale_threshold = 30
        
        for status in workflow_stages[:-1]:
            stage_issues = df[df['project_status'] == status]
            if not stage_issues.empty:
                stale_issues = stage_issues[
                    [(today - created).days > stale_threshold for created in stage_issues['created_at']]
                ]
                if not stale_issues.empty:
                    print(f"  {status:<20}: {len(stale_issues):>2} stale issues")
                    for _, issue in stale_issues.head(3).iterrows():  # Show top 3
                        age = (today - issue['created_at']).days
                        print(f"    #{issue['issue_number']}: {issue['title'][:50]}... ({age} days)")
        
        # 7. Workflow Efficiency Recommendations
        print("\n💡 Workflow Efficiency Recommendations:")
        
        # Check for bottlenecks
        in_progress = status_counts.get('Dev In Progress', 0)
        code_review = status_counts.get('Code Review', 0)
        backlog = status_counts.get('Dev Backlog', 0)
        
        if code_review > in_progress * 0.5:
            print("  🔍 Code Review bottleneck: Consider more reviewers or pair programming")
        
        if backlog > (in_progress + code_review) * 2:
            print("  📋 Large backlog: Prioritize and break down large items")
        
        if unassigned_count > assigned_count * 0.3:
            print("  👤 High unassigned work: Improve assignment and capacity planning")
        
        # Check for stale work
        total_stale = sum(
            len(df[(df['project_status'] == status) & 
                   [(today - created).days > stale_threshold for created in df['created_at']]])
            for status in workflow_stages[:-1]
        )
        
        if total_stale > len(df) * 0.1:
            print(f"  🕰️  {total_stale} stale issues: Review and close or re-prioritize old work")
        
        return df
    
    def _get_product_area_from_labels(self, labels):
        """Extract product area from issue labels"""
        labels_lower = [label.lower() for label in labels]
        
        for label in labels_lower:
            if label == 'product/ai':
                return 'AI Agent'
            elif label == 'product/voice':
                return 'Call Fabric'
            elif label == 'product/messaging':
                return 'Messaging'
            elif label == 'product/platform':
                return 'Spaces/Platform'
            elif label == 'product/ucaas':
                return 'PUC & SDK'
            elif label == 'product/video':
                return 'Video'
            elif label == 'project/data-zones':
                return 'Data Zones'
        
        return 'Other'
    
    def load_cycle_data_from_json(self, json_file_path: str) -> List[Dict]:
        """Load cycle time data from JSON file"""
        with open(json_file_path, 'r') as f:
            return json.load(f)
    
    def _create_workflow_visualization(self, workflow_data: List[Dict], output_dir: str):
        """Create workflow visualization with 4 panels"""
        if not workflow_data:
            return
        
        import pandas as pd
        import numpy as np
        df = pd.DataFrame(workflow_data)
        
        if df.empty:
            return
        
        # Set up the plot
        plt.style.use('seaborn-v0_8')
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'GitHub Projects Workflow Analysis - {self.owner}/{self.repo}', fontsize=14, fontweight='bold')
        
        # 1. Status Distribution
        status_counts = df['project_status'].value_counts()
        ax1.bar(range(len(status_counts)), status_counts.values)
        ax1.set_xticks(range(len(status_counts)))
        ax1.set_xticklabels(status_counts.index, rotation=45, ha='right')
        ax1.set_title('Work Distribution by Status')
        ax1.set_ylabel('Number of Issues')
        
        # 2. Product Area Distribution
        product_counts = df['product_area'].value_counts()
        ax2.pie(product_counts.values, labels=product_counts.index, autopct='%1.1f%%')
        ax2.set_title('Work Distribution by Product Area')
        
        # 3. Age by Status
        today = datetime.now(timezone.utc)
        workflow_stages = ['Dev Backlog', 'Dev In Progress', 'Code Review', 'To Deploy', 'Verify in Production']
        
        age_data = []
        for status in workflow_stages:
            stage_issues = df[df['project_status'] == status]
            if not stage_issues.empty:
                ages = [(today - created).days for created in stage_issues['created_at']]
                age_data.extend([(status, age) for age in ages])
        
        if age_data:
            age_df = pd.DataFrame(age_data, columns=['Status', 'Age_Days'])
            import seaborn as sns
            sns.boxplot(data=age_df, x='Status', y='Age_Days', ax=ax3)
            ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right')
            ax3.set_title('Work Age Distribution by Status')
            ax3.set_ylabel('Days Since Created')
        
        # 4. Assignment Status
        assignment_data = df.groupby('project_status')['assignee'].apply(lambda x: x.notna().sum()).reindex(workflow_stages, fill_value=0)
        unassigned_data = df.groupby('project_status')['assignee'].apply(lambda x: x.isna().sum()).reindex(workflow_stages, fill_value=0)
        
        x = range(len(workflow_stages))
        ax4.bar(x, assignment_data.values, label='Assigned', alpha=0.7)
        ax4.bar(x, unassigned_data.values, bottom=assignment_data.values, label='Unassigned', alpha=0.7)
        ax4.set_xticks(x)
        ax4.set_xticklabels(workflow_stages, rotation=45, ha='right')
        ax4.set_title('Assignment Status by Workflow Stage')
        ax4.set_ylabel('Number of Issues')
        ax4.legend()
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/workflow_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"✅ Workflow visualization saved to: {output_dir}/workflow_analysis.png")
        return df

    def _generate_workflow_section(self, workflow_analysis):
        """Generate HTML section for workflow analysis"""
        if not workflow_analysis:
            return ""
        
        # Status distribution table
        status_rows = ""
        for stage in workflow_analysis['workflow_stages']:
            stage_data = workflow_analysis['status_distribution'][stage]
            status_rows += f"""
            <tr>
                <td>{stage}</td>
                <td>{stage_data['count']}</td>
                <td>{stage_data['percentage']:.1f}%</td>
            </tr>"""
        
        # Bottlenecks section
        bottlenecks_html = ""
        if workflow_analysis['bottlenecks']:
            bottlenecks_html = "<h3>🚨 Workflow Bottlenecks</h3><ul>"
            for bottleneck in workflow_analysis['bottlenecks']:
                bottlenecks_html += f"<li><strong>{bottleneck['stage']}</strong>: {bottleneck['count']} issues ({bottleneck['percentage']:.1f}% of total work)</li>"
            bottlenecks_html += "</ul>"
        else:
            bottlenecks_html = "<h3>✅ No Major Bottlenecks Detected</h3><p>Work is well-distributed across workflow stages.</p>"
        
        # Age analysis table
        age_rows = ""
        for stage, data in workflow_analysis['age_analysis'].items():
            age_rows += f"""
            <tr>
                <td>{stage}</td>
                <td>{data['avg_age']:.1f} days</td>
                <td>{data['max_age']} days</td>
                <td>{data['stale_count']}</td>
            </tr>"""
        
        # Assignment analysis
        assignment = workflow_analysis['assignment']
        assignment_html = f"""
        <h3>👥 Work Assignment Analysis</h3>
        <ul>
            <li><strong>Assigned:</strong> {assignment['assigned']} issues ({100-assignment['unassigned_percentage']:.1f}%)</li>
            <li><strong>Unassigned:</strong> {assignment['unassigned']} issues ({assignment['unassigned_percentage']:.1f}%)</li>
        </ul>
        """
        
        if assignment['unassigned_percentage'] > 30:
            assignment_html += "<p><strong>⚠️ Warning:</strong> High percentage of unassigned work may indicate capacity planning issues.</p>"
        
        return f"""
        <h2>📊 GitHub Projects Workflow Analysis</h2>
        <p>Analysis of {workflow_analysis['total_issues']} open issues with project status tracking.</p>
        
        <h3>Work Distribution by Workflow Stage</h3>
        <table border="1" style="border-collapse: collapse; width: 100%;">
            <tr style="background-color: #f0f0f0;">
                <th style="padding: 8px;">Stage</th>
                <th style="padding: 8px;">Count</th>
                <th style="padding: 8px;">Percentage</th>
            </tr>
            {status_rows}
        </table>
        
        {bottlenecks_html}
        
        <h3>⏰ Work Age Analysis</h3>
        <table border="1" style="border-collapse: collapse; width: 100%;">
            <tr style="background-color: #f0f0f0;">
                <th style="padding: 8px;">Stage</th>
                <th style="padding: 8px;">Average Age</th>
                <th style="padding: 8px;">Oldest Item</th>
                <th style="padding: 8px;">Stale Items (>30 days)</th>
            </tr>
            {age_rows}
        </table>
        
        {assignment_html}
        """

    def _create_stage_progression_chart(self, df: pd.DataFrame, metrics: List, ax):
        """Create a stacked bar chart showing time spent in different phases by time period"""
        closed_issues = df[df['state'] == 'closed'].copy()
        
        if len(closed_issues) == 0:
            ax.text(0.5, 0.5, 'No closed issues with stage data', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Stage Progression Analysis')
            return
        
        # Extract month-year from closed_at for grouping
        closed_issues['month_year'] = pd.to_datetime(closed_issues['closed_at']).dt.to_period('M')
        
        # Get the metrics objects to access stage_segments
        metrics_by_issue = {}
        total_metrics = len(metrics)
        metrics_with_stages = 0
        for metric in metrics:
            if metric.stage_segments and metric.state == 'closed':
                metrics_by_issue[metric.issue_number] = metric
                metrics_with_stages += 1
        
        print(f"DEBUG: Total metrics: {total_metrics}, with stage segments: {metrics_with_stages}, closed with stages: {len(metrics_by_issue)}")
        
        # Aggregate stage data by month
        monthly_stage_data = {}
        
        for _, issue in closed_issues.iterrows():
            month = issue['month_year']
            issue_number = issue['issue_number']
            
            if issue_number in metrics_by_issue:
                metric = metrics_by_issue[issue_number]
                
                if month not in monthly_stage_data:
                    monthly_stage_data[month] = {}
                
                for segment in metric.stage_segments:
                    stage_name = segment.stage_name
                    duration = segment.duration_days or 0
                    
                    if stage_name not in monthly_stage_data[month]:
                        monthly_stage_data[month][stage_name] = 0
                    monthly_stage_data[month][stage_name] += duration
        
        if not monthly_stage_data:
            ax.text(0.5, 0.5, 'No stage progression data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Stage Progression Analysis')
            return
        
        # Convert to DataFrame for easier plotting
        stage_df = pd.DataFrame(monthly_stage_data).T.fillna(0)
        
        # Sort months chronologically
        stage_df = stage_df.sort_index()
        
        # Define colors for different stage types
        stage_colors = {
            'Requirements Review': '#ff9999',
            'Planning & Assignment': '#ffcc99', 
            'Development Planning': '#99ff99',
            'Development': '#99ff99',
            'Active Development': '#99ff99',
            'Development & Deployment': '#99ff99',
            'Development & Integration': '#99ff99',
            'Code Review & Deployment': '#9999ff',
            'Complete Lifecycle': '#cccccc',
            'Planning & Development': '#ffff99'
        }
        
        # Create stacked bar chart
        bottom = None
        month_labels = [str(month) for month in stage_df.index]
        
        for stage in stage_df.columns:
            color = stage_colors.get(stage, '#cccccc')
            values = stage_df[stage]
            
            ax.bar(month_labels, values, bottom=bottom, 
                  label=stage, color=color, alpha=0.8)
            
            if bottom is None:
                bottom = values
            else:
                bottom += values
        
        ax.set_title('Time Spent in Different Phases by Month')
        ax.set_xlabel('Month')
        ax.set_ylabel('Total Days')
        ax.tick_params(axis='x', rotation=45)
        
        # Add legend (limit to avoid overcrowding)
        handles, labels = ax.get_legend_handles_labels()
        if len(labels) <= 6:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
        else:
            # Show only top 6 stages by total time
            stage_totals = stage_df.sum().sort_values(ascending=False)
            top_stages = stage_totals.head(6).index
            filtered_handles = [handles[labels.index(stage)] for stage in top_stages if stage in labels]
            filtered_labels = [stage for stage in top_stages if stage in labels]
            ax.legend(filtered_handles, filtered_labels, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
    
    def _create_detailed_stage_breakdown_chart(self, timeline_df: pd.DataFrame, ax):
        """Create a detailed stacked bar chart showing stage breakdown by issue type"""
        # Get all stage columns (exclude standard columns)
        standard_cols = ['issue_type', 'queue_time', 'work_time', 'total_time', 'closure_month', 'efficiency_ratio']
        stage_columns = [col for col in timeline_df.columns if col not in standard_cols]
        
        if not stage_columns:
            ax.text(0.5, 0.5, 'No detailed stage data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Stage Breakdown by Issue Type')
            return
        
        # Aggregate stage data by issue type
        issue_type_summary = timeline_df.groupby('issue_type')[stage_columns].mean()
        issue_type_counts = timeline_df['issue_type'].value_counts()
        
        # Filter to issue types with at least 5 issues
        valid_types = issue_type_counts[issue_type_counts >= 5].index
        issue_type_summary = issue_type_summary.loc[valid_types]
        
        if issue_type_summary.empty:
            ax.text(0.5, 0.5, 'Insufficient data for stage breakdown', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Stage Breakdown by Issue Type')
            return
        
        # Define colors for different stages
        stage_colors = {
            'Requirements Review': '#ff9999',
            'Planning & Assignment': '#ffcc99', 
            'Development Planning': '#99ff99',
            'Development': '#99ff99',
            'Active Development': '#99ff99',
            'Development & Deployment': '#99ff99',
            'Development & Integration': '#99ff99',
            'Code Review & Deployment': '#9999ff',
            'Complete Lifecycle': '#cccccc',
            'Planning & Development': '#ffff99'
        }
        
        # Sort columns by total time across all issue types (most significant first)
        column_totals = issue_type_summary.sum().sort_values(ascending=False)
        sorted_columns = column_totals.head(8).index.tolist()  # Limit to top 8 stages
        
        # Create stacked bar chart
        bottom = None
        issue_types = issue_type_summary.index.tolist()
        
        for stage in sorted_columns:
            if stage in issue_type_summary.columns:
                color = stage_colors.get(stage, '#cccccc')
                values = issue_type_summary[stage]
                
                ax.bar(issue_types, values, bottom=bottom, 
                      label=stage, color=color, alpha=0.8)
                
                if bottom is None:
                    bottom = values
                else:
                    bottom += values
        
        ax.set_title('Average Stage Breakdown by Issue Type (All Time)')
        ax.set_xlabel('Issue Type')
        ax.set_ylabel('Average Days')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
        
        # Add count annotations
        for i, issue_type in enumerate(issue_types):
            if issue_type in issue_type_counts:
                count = issue_type_counts[issue_type]
                total_height = bottom.iloc[i] if bottom is not None else 0
                ax.annotate(f'n={count}', 
                           xy=(i, total_height), xytext=(0, 5),
                           textcoords='offset points', ha='center', va='bottom',
                           fontsize=9, color='black')
        
        # Add legend (limit entries)
        handles, labels = ax.get_legend_handles_labels()
        if len(labels) <= 6:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
        else:
            # Show only top 6 stages
            ax.legend(handles[:6], labels[:6], bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')

    def _create_monthly_stage_progression_chart(self, timeline_df: pd.DataFrame, recent_data: pd.DataFrame, ax):
        """Create monthly stage progression stacked bar chart showing wait -> work -> wait -> work progression"""
        if recent_data.empty:
            ax.text(0.5, 0.5, 'No recent data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Monthly Stage Progression (Last 12 Months)')
            return
        
        # Get all stage columns (exclude standard columns)
        standard_cols = ['issue_type', 'queue_time', 'work_time', 'total_time', 'closure_month', 'efficiency_ratio']
        stage_columns = [col for col in recent_data.columns if col not in standard_cols]
        
        print(f"DEBUG: Timeline data columns: {list(recent_data.columns)}")
        print(f"DEBUG: Stage columns found: {stage_columns}")
        
        if not stage_columns:
            ax.text(0.5, 0.5, 'No stage data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Monthly Stage Progression (Last 12 Months)')
            return
        
        # Aggregate stage data by month
        monthly_stage_data = recent_data.groupby('closure_month')[stage_columns].mean()
        
        # Sort by chronological order
        monthly_stage_data = monthly_stage_data.sort_index()
        
        # Define colors for different stages
        stage_colors = {
            'Requirements Review': '#ff9999',
            'Planning & Assignment': '#ffcc99', 
            'Development Planning': '#99ff99',
            'Development': '#99ff99',
            'Active Development': '#99ff99',
            'Development & Deployment': '#99ff99',
            'Development & Integration': '#99ff99',
            'Code Review & Deployment': '#9999ff',
            'Complete Lifecycle': '#cccccc',
            'Planning & Development': '#ffff99'
        }
        
        # Sort stages by total time (most significant first)
        stage_totals = monthly_stage_data.sum().sort_values(ascending=False)
        sorted_stages = stage_totals.head(8).index.tolist()  # Top 8 stages
        
        # Create stacked bar chart
        bottom = None
        month_labels = [str(month) for month in monthly_stage_data.index]
        
        for stage in sorted_stages:
            if stage in monthly_stage_data.columns:
                color = stage_colors.get(stage, '#cccccc')
                values = monthly_stage_data[stage]
                
                ax.bar(month_labels, values, bottom=bottom, 
                      label=stage, color=color, alpha=0.8)
                
                if bottom is None:
                    bottom = values
                else:
                    bottom += values
        
        ax.set_title('Monthly Stage Progression (Last 12 Months)')
        ax.set_xlabel('Month')
        ax.set_ylabel('Average Days per Stage')
        ax.tick_params(axis='x', rotation=45)
        ax.grid(True, alpha=0.3)
        
        # Add legend (limit entries)
        handles, labels = ax.get_legend_handles_labels()
        if len(labels) <= 6:
            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')
        else:
            ax.legend(handles[:6], labels[:6], bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')

    def _create_monthly_stage_by_type_chart(self, timeline_df: pd.DataFrame, recent_data: pd.DataFrame, ax):
        """Create monthly stage progression by issue type stacked bar chart"""
        if recent_data.empty:
            ax.text(0.5, 0.5, 'No recent data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Monthly Stage Progression by Issue Type (Last 12 Months)')
            return
        
        # Get top issue types for focus
        top_types = timeline_df['issue_type'].value_counts().head(3).index.tolist()
        
        if not top_types:
            ax.text(0.5, 0.5, 'No issue type data available', 
                   horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)
            ax.set_title('Monthly Stage Progression by Issue Type (Last 12 Months)')
            return
        
        # Filter to top issue types and aggregate by month
        filtered_data = recent_data[recent_data['issue_type'].isin(top_types)]
        monthly_type_data = filtered_data.groupby(['closure_month', 'issue_type']).agg({
            'queue_time': 'mean',
            'work_time': 'mean'
        }).reset_index()
        
        # Get unique months
        months = sorted(monthly_type_data['closure_month'].unique())
        month_labels = [str(month) for month in months]
        
        # Create grouped stacked bars
        bar_width = 0.25
        x_positions = np.arange(len(months))
        
        # Colors for work vs wait
        wait_color = '#ff7f7f'
        work_color = '#7fbf7f'
        
        for i, issue_type in enumerate(top_types):
            type_data = monthly_type_data[monthly_type_data['issue_type'] == issue_type]
            
            # Align data with months
            wait_values = []
            work_values = []
            
            for month in months:
                month_data = type_data[type_data['closure_month'] == month]
                if not month_data.empty:
                    wait_values.append(month_data['queue_time'].iloc[0])
                    work_values.append(month_data['work_time'].iloc[0])
                else:
                    wait_values.append(0)
                    work_values.append(0)
            
            # Position bars for this issue type
            pos = x_positions + (i - 1) * bar_width
            
            # Create stacked bars
            ax.bar(pos, wait_values, bar_width, label=f'{issue_type} (Wait)' if i == 0 else '', 
                  color=wait_color, alpha=0.8)
            ax.bar(pos, work_values, bar_width, bottom=wait_values, 
                  label=f'{issue_type} (Work)' if i == 0 else '', color=work_color, alpha=0.8)
            
            # Add issue type labels below bars
            if i == 1:  # Middle position
                for j, pos_val in enumerate(pos):
                    total_height = wait_values[j] + work_values[j]
                    if total_height > 0:
                        ax.text(pos_val, -total_height * 0.1, issue_type, 
                               rotation=45, ha='center', va='top', fontsize=8)
        
        ax.set_title('Monthly Work vs Wait Time by Issue Type (Last 12 Months)')
        ax.set_xlabel('Month')
        ax.set_ylabel('Average Days')
        ax.set_xticks(x_positions)
        ax.set_xticklabels(month_labels, rotation=45)
        ax.grid(True, alpha=0.3)
        
        # Custom legend
        wait_patch = plt.Rectangle((0,0),1,1, color=wait_color, alpha=0.8)
        work_patch = plt.Rectangle((0,0),1,1, color=work_color, alpha=0.8)
        ax.legend([wait_patch, work_patch], ['Wait Time', 'Work Time'], 
                 bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small')

    def generate_report(self, metrics: List[CycleTimeMetrics], output_dir: str = "cycle_time_report"):
        """Generate comprehensive cycle time report"""
        Path(output_dir).mkdir(exist_ok=True)
        
        # Store metrics for access by chart functions
        self.last_analyzed_metrics = metrics
        
        try:
            
            # Analyze project workflow if we have project data
            workflow_analysis = self.analyze_project_workflow(metrics)
            
            # Convert to DataFrame for analysis
            df_data = []
            for metric in metrics:
                # Create stage progression summary for CSV
                stage_progression = ""
                if metric.stage_segments:
                    stage_names = [seg.stage_name for seg in metric.stage_segments]
                    stage_progression = " → ".join(stage_names)
                
                df_data.append({
                    'issue_number': metric.issue_number,
                    'title': metric.title,
                    'created_at': metric.created_at,
                    'closed_at': metric.closed_at,
                    'work_started_at': metric.work_started_at,
                    'lead_time_days': metric.lead_time_days,
                    'cycle_time_days': metric.cycle_time_days,
                    'labels': ', '.join(metric.labels),
                    'assignee': metric.assignee,
                    'milestone': metric.milestone,
                    'state': metric.state,
                    'comments': 0,  # Default value, could be enhanced later
                    'total_work_time_days': metric.total_work_time_days,
                    'total_wait_time_days': metric.total_wait_time_days,
                    'work_efficiency_ratio': metric.work_efficiency_ratio,
                    'stage_progression': stage_progression
                })
            
            df = pd.DataFrame(df_data)
            
            
            # Note: Data files (JSON/CSV) are now created by sync_issues.py
            # This script focuses on analysis and report generation
            
            # Generate summary statistics
            closed_issues = df[df['state'] == 'closed']
            
            if len(closed_issues) == 0:
                print("No closed issues found for analysis - generating report with available data")
                # Still generate a basic report even without closed issues
            
            
            # Calculate statistics (only if we have closed issues)
            if len(closed_issues) > 0:
                lead_time_stats = closed_issues['lead_time_days'].describe()
                cycle_time_data_valid = closed_issues['cycle_time_days'].dropna()
                if len(cycle_time_data_valid) > 0:
                    cycle_time_stats = cycle_time_data_valid.describe()
                else:
                    cycle_time_stats = pd.Series(dtype='float64')
            else:
                # Create empty series for when there are no closed issues
                lead_time_stats = pd.Series(dtype='float64')
                cycle_time_stats = pd.Series(dtype='float64')
            
            # Calculate monthly cycle time trend with rolling 6-month average
            monthly_cycle_data = self._calculate_monthly_cycle_trends(closed_issues)
            
            # Generate advanced analyses
            segment_analysis = self._analyze_cycle_time_segments(df)
            assignment_analysis = self._analyze_assignment_patterns(df)
            status_analysis = self._analyze_status_progression(df)
            
            
            # Generate visualizations
            plt.style.use('seaborn-v0_8')
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Cycle Time Analysis for {self.owner}/{self.repo}', fontsize=16)
            
            # Lead time distribution
            axes[0, 0].hist(closed_issues['lead_time_days'].dropna(), bins=20, alpha=0.7, color='skyblue')
            axes[0, 0].set_title('Lead Time Distribution (Days)')
            axes[0, 0].set_xlabel('Days')
            axes[0, 0].set_ylabel('Frequency')
            
            # Cycle time distribution
            cycle_times = closed_issues['cycle_time_days'].dropna()
            if len(cycle_times) > 0:
                axes[0, 1].hist(cycle_times, bins=20, alpha=0.7, color='lightgreen')
                axes[0, 1].set_title('Cycle Time Distribution (Days)')
                axes[0, 1].set_xlabel('Days')
                axes[0, 1].set_ylabel('Frequency')
            
            # Monthly cycle time trend with 6-month rolling average
            if not monthly_cycle_data.empty:
                axes[1, 0].plot(monthly_cycle_data.index, monthly_cycle_data['monthly_avg'], 
                               alpha=0.7, marker='o', markersize=4, label='Monthly Average', color='orange')
                axes[1, 0].plot(monthly_cycle_data.index, monthly_cycle_data['rolling_6m'], 
                               linewidth=2, label='6-Month Rolling Average', color='red')
                axes[1, 0].set_title('Monthly Cycle Time Trend')
                axes[1, 0].set_xlabel('Month')
                axes[1, 0].set_ylabel('Cycle Time (Days)')
                axes[1, 0].legend()
                axes[1, 0].tick_params(axis='x', rotation=45)
                axes[1, 0].grid(True, alpha=0.3)
            
            # Stage progression stacked bar chart
            self._create_stage_progression_chart(df, metrics, axes[1, 1])
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/cycle_time_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            # Generate timeline visualization
            timeline_data = self._create_timeline_visualization(df, output_dir)
            
            # Generate workflow visualization if we have project data
            if workflow_analysis and workflow_analysis.get('workflow_data'):
                self._create_workflow_visualization(workflow_analysis['workflow_data'], output_dir)
            
            
            # Generate AI recommendations
            recommendations = self._generate_ai_recommendations(df, lead_time_stats, cycle_time_stats, monthly_cycle_data)
            
            
            # Generate HTML report
            html_report = self._generate_html_report(
                closed_issues, lead_time_stats, cycle_time_stats, monthly_cycle_data, 
                segment_analysis, assignment_analysis, status_analysis, recommendations, 
                workflow_analysis, output_dir
            )
            
            with open(f"{output_dir}/cycle_time_report.html", 'w') as f:
                f.write(html_report)
            
            print(f"\nReport generated in '{output_dir}' directory:")
            print(f"- cycle_time_analysis.png: Basic visualizations")
            print(f"- timeline_analysis.png: Stage progression timeline analysis")
            if workflow_analysis:
                print(f"- workflow_analysis.png: GitHub Projects workflow analysis")
            print(f"- cycle_time_report.html: Full HTML report with workflow insights")
            
            # Print summary to console
            print(f"\n=== CYCLE TIME ANALYSIS SUMMARY ===")
            print(f"Repository: {self.owner}/{self.repo}")
            print(f"Total Issues: {len(df)}")
            print(f"Closed Issues: {len(closed_issues)}")
            
            if len(closed_issues) > 0 and not lead_time_stats.empty:
                print(f"\nLead Time Statistics (days):")
                print(f"  Average: {lead_time_stats['mean']:.1f}")
                print(f"  Median: {lead_time_stats['50%']:.1f}")
                print(f"  Min: {lead_time_stats['min']:.1f}")
                print(f"  Max: {lead_time_stats['max']:.1f}")
            else:
                print(f"\nLead Time Statistics: No closed issues available")
            
            if not cycle_time_stats.empty:
                print(f"\nCycle Time Statistics (days):")
                print(f"  Average: {cycle_time_stats['mean']:.1f}")
                print(f"  Median: {cycle_time_stats['50%']:.1f}")
                print(f"  Min: {cycle_time_stats['min']:.1f}")
                print(f"  Max: {cycle_time_stats['max']:.1f}")
                
                # Work/Wait Time Analysis Summary
                work_time_data = closed_issues['total_work_time_days'].dropna()
                wait_time_data = closed_issues['total_wait_time_days'].dropna()
                efficiency_data = closed_issues['work_efficiency_ratio'].dropna()
                
                if not work_time_data.empty and not wait_time_data.empty:
                    print(f"\nWork vs Wait Time Analysis:")
                    print(f"  Average Work Time: {work_time_data.mean():.1f} days")
                    print(f"  Average Wait Time: {wait_time_data.mean():.1f} days")
                    if not efficiency_data.empty:
                        print(f"  Work Efficiency (Work/Total): {efficiency_data.mean()*100:.1f}%")
                    print(f"  Issues with Work/Wait data: {len(efficiency_data)}")
            else:
                print(f"\nCycle Time Statistics: No closed issues with cycle time data available")
        
        except Exception as e:
            print(f"Error generating report: {e}")
    
    def _generate_html_report(self, df, lead_time_stats, cycle_time_stats, monthly_cycle_data, 
                             segment_analysis, assignment_analysis, status_analysis, recommendations, 
                             workflow_analysis, output_dir):
        """Generate HTML report"""
        return f"""
<!DOCTYPE html>
<html>
<head>
    <title>Cycle Time Report - {self.owner}/{self.repo}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .header {{ background-color: #f5f5f5; padding: 20px; border-radius: 8px; }}
        .metric {{ background-color: #e8f4fd; padding: 15px; margin: 10px 0; border-radius: 5px; }}
        .chart {{ text-align: center; margin: 20px 0; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Cycle Time Analysis Report</h1>
        <p><strong>Repository:</strong> {self.owner}/{self.repo}</p>
        <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <h2>Executive Summary</h2>
    <div class="metric">
        <h3>Lead Time (Creation to Closure)</h3>
        <p><strong>Average:</strong> {f"{lead_time_stats['mean']:.1f}" if not lead_time_stats.empty else 'N/A'} days</p>
        <p><strong>Median:</strong> {f"{lead_time_stats['50%']:.1f}" if not lead_time_stats.empty else 'N/A'} days</p>
        <p><strong>90th Percentile:</strong> {f"{lead_time_stats['90%']:.1f}" if not lead_time_stats.empty and '90%' in lead_time_stats else 'N/A'} days</p>
    </div>
    
    <div class="metric">
        <h3>Cycle Time (Work Start to Closure)</h3>
        <p><strong>Average:</strong> {f"{cycle_time_stats['mean']:.1f}" if not cycle_time_stats.empty else 'N/A'} days</p>
        <p><strong>Median:</strong> {f"{cycle_time_stats['50%']:.1f}" if not cycle_time_stats.empty else 'N/A'} days</p>
    </div>
    
    {f'''<div class="metric">
        <h3>Monthly Cycle Time Trend (6-Month Rolling Average)</h3>
        <p><strong>Latest 6-Month Average:</strong> {monthly_cycle_data['rolling_6m'].iloc[-1]:.1f} days</p>
        <p><strong>Trend Direction:</strong> {
            "Improving" if len(monthly_cycle_data) >= 2 and monthly_cycle_data['rolling_6m'].iloc[-1] < monthly_cycle_data['rolling_6m'].iloc[-2] 
            else "Worsening" if len(monthly_cycle_data) >= 2 and monthly_cycle_data['rolling_6m'].iloc[-1] > monthly_cycle_data['rolling_6m'].iloc[-2]
            else "Stable"
        }</p>
        <p><strong>Data Points:</strong> {len(monthly_cycle_data)} months</p>
    </div>''' if not monthly_cycle_data.empty else ''}
    
    <div class="chart">
        <img src="cycle_time_analysis.png" alt="Cycle Time Analysis Charts" style="max-width: 100%;">
    </div>
    
    <div class="chart">
        <img src="timeline_analysis.png" alt="Timeline Analysis" style="max-width: 100%;">
    </div>
    
    {f'<div class="chart"><img src="workflow_analysis.png" alt="Workflow Analysis" style="max-width: 100%;"></div>' if workflow_analysis else ''}
    
    <h2>Workflow Analysis</h2>
    
    {f'''<div class="metric">
        <h3>Cycle Time by Issue Type</h3>
        <table>
            <tr><th>Type</th><th>Count</th><th>Mean Days</th><th>Median Days</th></tr>
            {''.join(f"<tr><td>{type_name}</td><td>{data['count']}</td><td>{data['mean']}</td><td>{data['median']}</td></tr>" 
                    for type_name, data in segment_analysis.get('by_issue_type', {}).items())}
        </table>
    </div>''' if segment_analysis.get('by_issue_type') else ''}
    
    {f'''<div class="metric">
        <h3>Cycle Time by Team</h3>
        <table>
            <tr><th>Team</th><th>Count</th><th>Mean Days</th><th>Median Days</th></tr>
            {''.join(f"<tr><td>{team}</td><td>{data['count']}</td><td>{data['mean']}</td><td>{data['median']}</td></tr>" 
                    for team, data in segment_analysis.get('by_team', {}).items())}
        </table>
    </div>''' if segment_analysis.get('by_team') else ''}
    
    {f'''<div class="metric">
        <h3>Cycle Time by Product Area</h3>
        <table>
            <tr><th>Product</th><th>Count</th><th>Mean Days</th><th>Median Days</th></tr>
            {''.join(f"<tr><td>{product}</td><td>{data['count']}</td><td>{data['mean']}</td><td>{data['median']}</td></tr>" 
                    for product, data in segment_analysis.get('by_product_area', {}).items())}
        </table>
    </div>''' if segment_analysis.get('by_product_area') else ''}
    
    {f'''<div class="metric">
        <h3>Cycle Time by Priority</h3>
        <table>
            <tr><th>Priority</th><th>Count</th><th>Mean Days</th><th>Median Days</th></tr>
            {''.join(f"<tr><td>{priority}</td><td>{data['count']}</td><td>{data['mean']}</td><td>{data['median']}</td></tr>" 
                    for priority, data in segment_analysis.get('by_priority', {}).items())}
        </table>
    </div>''' if segment_analysis.get('by_priority') else ''}
    
    <h2>Assignment & Queue Analysis</h2>
    
    <div class="metric">
        <h3>Time to Assignment</h3>
        <p><strong>Average:</strong> {assignment_analysis.get('time_to_assignment', {}).get('mean_days', 'N/A')} days</p>
        <p><strong>Median:</strong> {assignment_analysis.get('time_to_assignment', {}).get('median_days', 'N/A')} days</p>
        <p><strong>Max:</strong> {assignment_analysis.get('time_to_assignment', {}).get('max_days', 'N/A')} days</p>
        <p><strong>Issues analyzed:</strong> {assignment_analysis.get('time_to_assignment', {}).get('count', 0)}</p>
    </div>
    
    <div class="metric">
        <h3>Assignment Stability</h3>
        <p><strong>Average reassignments per issue:</strong> {assignment_analysis.get('assignment_stability', {}).get('mean_reassignments', 'N/A')}</p>
        <p><strong>Issues with reassignments:</strong> {assignment_analysis.get('assignment_stability', {}).get('issues_with_reassignments', 0)}</p>
        <p><strong>Stability rate:</strong> {assignment_analysis.get('assignment_stability', {}).get('stability_rate', 'N/A')}%</p>
    </div>
    
    <div class="metric">
        <h3>Team Collaboration</h3>
        <p><strong>Multi-assignee issues:</strong> {assignment_analysis.get('team_collaboration', {}).get('multi_assignee_issues', 0)}</p>
        <p><strong>Collaboration rate:</strong> {assignment_analysis.get('team_collaboration', {}).get('collaboration_rate', 0)}%</p>
    </div>
    
    <h2>Status & Queue Times</h2>
    
    <div class="metric">
        <h3>Queue Time (Creation to Work Start)</h3>
        <p><strong>Average:</strong> {status_analysis.get('queue_time', {}).get('mean_days', 'N/A')} days</p>
        <p><strong>Median:</strong> {status_analysis.get('queue_time', {}).get('median_days', 'N/A')} days</p>
        <p><strong>Max:</strong> {status_analysis.get('queue_time', {}).get('max_days', 'N/A')} days</p>
    </div>
    
    <div class="metric">
        <h3>Time in Review Status</h3>
        <p><strong>Average:</strong> {status_analysis.get('needs_review_time', {}).get('mean_days', 'N/A')} days</p>
        <p><strong>Median:</strong> {status_analysis.get('needs_review_time', {}).get('median_days', 'N/A')} days</p>
        <p><strong>Max:</strong> {status_analysis.get('needs_review_time', {}).get('max_days', 'N/A')} days</p>
    </div>
    
    <h2>Key Insights</h2>
    <ul>
        <li>Total issues analyzed: {len(df)}</li>
        <li>Issues with calculable cycle time: {len(df[df['cycle_time_days'].notna()])}</li>
        <li>Average time from creation to work start: {f"{((df['work_started_at'] - df['created_at']).dt.total_seconds() / (24*3600)).mean():.1f}" if df['work_started_at'].notna().any() and not df.empty else 'N/A'} days</li>
    </ul>
    
    {self._generate_workflow_section(workflow_analysis) if workflow_analysis else ''}
    
    <h2>Recommendations</h2>
    <ul>
        {chr(10).join(f"<li>{rec}</li>" for rec in recommendations)}
    </ul>
    {f'<p><em>Recommendations generated using AI analysis of repository data.</em></p>' if os.getenv('OPENAI_API_KEY') else '<p><em>Set OPENAI_API_KEY and OPENAI_MODEL environment variables for AI-generated recommendations.</em></p>'}
    
    <p><em>For detailed data, see the JSON file used as input to this analysis</em></p>
</body>
</html>
        """

def main():
    """Main execution function"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Analyze GitHub repository cycle times from JSON data',
        epilog='''
Data Source:
  This tool analyzes GitHub issues data from JSON files generated by sync_issues.py.
  Run sync_issues.py first to collect the data, then use this tool to analyze it.

Strategic Work Focus:
  Analysis focuses on strategic business value work only:
  INCLUDES: product features, customer issues, epics
  EXCLUDES: chores, deployments, infrastructure tasks
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('json_file', help='JSON file with issues data (generated by sync_issues.py)')
    parser.add_argument('--fast', action='store_true', help='Skip work start detection for faster processing (only basic lead times)')
    parser.add_argument('--workflow-analysis', action='store_true', help='Run detailed workflow analysis with console output')
    args = parser.parse_args()
    
    # Check if JSON file exists
    if not os.path.exists(args.json_file):
        print(f"Error: JSON file '{args.json_file}' not found.")
        print("\nTo create the JSON file, first run:")
        print("  python sync_issues.py <owner> <repo>")
        return
    
    # Load environment variables from .env file
    load_dotenv()
    
    # Optional AI recommendations
    openai_key = os.getenv('OPENAI_API_KEY')
    openai_model = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')
    console = Console() if RICH_AVAILABLE else None
    
    if console:
        if openai_key:
            console.print(f"🤖 AI recommendations enabled using model: {openai_model}", style="green")
        else:
            console.print("🤖 AI recommendations disabled. Set OPENAI_API_KEY environment variable to enable.", style="dim")
    else:
        if openai_key:
            print(f"AI recommendations enabled using model: {openai_model}")
        else:
            print("AI recommendations disabled. Set OPENAI_API_KEY environment variable to enable.")
    
    # Initialize analyzer
    analyzer = GitHubCycleTimeAnalyzer()
    
    try:
        print(f"Loading issues data from {args.json_file}...")
        json_data = analyzer.load_cycle_data_from_json(args.json_file)
        
        # Extract issues from JSON data
        if 'issues' in json_data:
            issues = json_data['issues']
        else:
            # Handle legacy format where JSON is just a list of issues
            issues = json_data if isinstance(json_data, list) else []
        
        if not issues:
            print("No issues found in JSON file")
            return
        
        
        print(f"📊 Processing {len(issues)} issues...")
        
        # Calculate cycle times
        metrics = analyzer.calculate_cycle_times(issues, fast_mode=args.fast)
        
        if not metrics:
            print("No metrics calculated - unable to generate report")
            return
        
        # Run detailed workflow analysis if requested
        if args.workflow_analysis:
            print("\n" + "=" * 60)
            print("DETAILED WORKFLOW ANALYSIS")
            print("=" * 60)
            analyzer.analyze_project_workflow_detailed(metrics)
            print("=" * 60)
        
        if console:
            console.print(f"📈 Generating report for {len(metrics)} issues...", style="blue bold")
        else:
            print(f"\nGenerating report for {len(metrics)} issues...")
        
        # Generate report
        analyzer.generate_report(metrics)
        
    except Exception as e:
        print(f"\n❌ An error occurred: {e}")
        return

if __name__ == "__main__":
    main()